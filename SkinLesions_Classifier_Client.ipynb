{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zKJVJouNgEG_"
      ],
      "authorship_tag": "ABX9TyMBeo0RQAuRMDIzdHnl6vC2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thaCripple/SkinLesion_Classifier_v1/blob/main/SkinLesions_Classifier_Client.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backend"
      ],
      "metadata": {
        "id": "zKJVJouNgEG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import json\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "3zfM0li7gKkf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "z9n01hPmfvAK"
      },
      "outputs": [],
      "source": [
        "import gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "UxovMdjvjxyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to the Google Drive Folder with the model and other data"
      ],
      "metadata": {
        "id": "KajVAbSHiUfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_URL = \"https://drive.google.com/drive/folders/18vhOdDM-rU21D8rztAiEkZ4013MORfIT?usp=sharing\""
      ],
      "metadata": {
        "id": "e5cLHeFjiJ7G"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class labels"
      ],
      "metadata": {
        "id": "8SRsChD6j6K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']"
      ],
      "metadata": {
        "id": "u7WvCBE4j06a"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CWD = Path.cwd()"
      ],
      "metadata": {
        "id": "iKpl1o4PhuD1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\""
      ],
      "metadata": {
        "id": "X358V_qRk2rL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Required Resources"
      ],
      "metadata": {
        "id": "91EojZuLmCZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the data folder from google drive"
      ],
      "metadata": {
        "id": "b39kF0bsjYTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder(url=DATA_URL, output=str(CWD), quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC41ldxHi7eU",
        "outputId": "80956014-baaa-4f62-d3c3-97f4638798cc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/lesions_database.json', '/content/skin_lesion_classifier_model.pth']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model"
      ],
      "metadata": {
        "id": "yflnINKzkZ5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = CWD / \"skin_lesion_classifier_model.pth\""
      ],
      "metadata": {
        "id": "00uRm0egkjiV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = torch.compile(torch.load(f=model_path, weights_only=False, map_location=torch.device(device)))"
      ],
      "metadata": {
        "id": "0BAZSR9vkF_D"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load image transforms"
      ],
      "metadata": {
        "id": "E9MQBL_PlVi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_transforms = torchvision.models.EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()"
      ],
      "metadata": {
        "id": "B36JDlR0lei8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Lesions Database"
      ],
      "metadata": {
        "id": "ROgM-TWuwXzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lesions_db_path = CWD / \"lesions_database.json\""
      ],
      "metadata": {
        "id": "pxGBv8wxwhWK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file=lesions_db_path, mode=\"r\") as file:\n",
        "  lesions_db = json.load(file)"
      ],
      "metadata": {
        "id": "VsF4RT6Rw1Vv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Workflow"
      ],
      "metadata": {
        "id": "zKAC-SeunVJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse and process the Images"
      ],
      "metadata": {
        "id": "XYLyAqxTmSxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parses only `.jpg` images for now - not sure if other formats will be necessary"
      ],
      "metadata": {
        "id": "VaNAR6w_m-Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_images()->list:\n",
        "  \"\"\"\n",
        "  Parses the session storage for .jpg files and converts them to PIL Images\n",
        "\n",
        "  Returns:\n",
        "  A list of PIL.Image files\n",
        "\n",
        "  Raises:\n",
        "  Exception when no images were found\n",
        "  \"\"\"\n",
        "  img_paths = list(CWD.glob(\"*.jpg\"))\n",
        "  if len(img_paths) < 1:\n",
        "    raise Exception(\"No suitable images found\")\n",
        "\n",
        "  imgs = []\n",
        "  for img_path in img_paths:\n",
        "    imgs.append(Image.open(fp=img_path))\n",
        "\n",
        "  return imgs"
      ],
      "metadata": {
        "id": "g7WUmEXkmUZp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_image(img:Image)->torch.Tensor:\n",
        "  \"\"\"\n",
        "  Performs transformations required for inference and adds a fake batch dimension of 1 to the output tensor\n",
        "\n",
        "  Parameters:\n",
        "    img: A single PIL.Image image\n",
        "\n",
        "  Returns:\n",
        "  A Tensor with the transformed image suitable for the model\n",
        "  \"\"\"\n",
        "  return torch.unsqueeze(input=img_transforms(img), dim=0).to(device=device)"
      ],
      "metadata": {
        "id": "GFlXcWVkoiKj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classify the Images"
      ],
      "metadata": {
        "id": "BTcSbn9GpTxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_img(transformed_img:torch.Tensor, model:nn.Module=classifier)->torch.Tensor:\n",
        "  \"\"\"\n",
        "  Performes inference on the passed image Tensor\n",
        "\n",
        "  Parameters:\n",
        "    transformed_img: Image tensor\n",
        "\n",
        "    model: Pytorch model used for classification\n",
        "\n",
        "  Returns:\n",
        "  Tensor with prediction probabilites\n",
        "  \"\"\"\n",
        "  classifier.eval()\n",
        "  with torch.inference_mode():\n",
        "    logits = classifier(transformed_img)\n",
        "    probabilities = nn.functional.softmax(input=logits, dim=1)\n",
        "  return probabilities"
      ],
      "metadata": {
        "id": "bnPVs6YIpYev"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform and present prediction - Main Function"
      ],
      "metadata": {
        "id": "QSviRqdCr8Wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diagnose_lesions()->None:\n",
        "  \"\"\"\n",
        "  The main function of the application.\n",
        "  Encapsulates all the previously defined functions.\n",
        "  1. Opens and transforms user uploaded images\n",
        "  2. Performs classification and extract classification statistics\n",
        "  3. Presents the classification to the user\n",
        "  \"\"\"\n",
        "\n",
        "  # Get a list of original images uploaded by the user\n",
        "  imgs_original = get_images()\n",
        "  img_count = len(imgs_original)\n",
        "\n",
        "  # Create a figure for prediction display\n",
        "  fig, ax = plt.subplots(nrows=img_count, ncols=2, figsize=(5*img_count, 10))\n",
        "\n",
        "  for i in range(img_count):\n",
        "    original_img = imgs_original[i]\n",
        "\n",
        "    # Transform the image for the model\n",
        "    img_transformed = transform_image(img=original_img)\n",
        "\n",
        "    # Classify the image\n",
        "    probabilities = classify_img(transformed_img=img_transformed)\n",
        "    label = LABELS[torch.argmax(input=probabilities)] # Symbolic label eg.: 'bcc'\n",
        "    confidence = round(probabilities.max().item()*100, 1) # Confidence of the classification\n",
        "    full_name = lesions_db.get(label)[\"name\"] # Full lesion's name\n",
        "    description = lesions_db.get(label)[\"info\"] # Lesion's description\n",
        "\n",
        "\n",
        "    # Get the 2nd most probable classification\n",
        "    second_classification = torch.sort(probabilities, descending=True)\n",
        "    second_confidence = round(second_classification.values[0,1].item()*100, 1)\n",
        "    second_label = LABELS[second_classification.indices[0,1]]\n",
        "    second_full_name = lesions_db.get(second_label)[\"name\"]\n",
        "    second_description = lesions_db.get(second_label)[\"info\"]\n",
        "\n",
        "\n",
        "    # Display the image\n",
        "    ax[i,0].imshow(original_img)\n",
        "    ax[i,0].axis(False)\n",
        "\n",
        "    # Display primary classification information\n",
        "    confidence_text = f\"The Model is {confidence}% certain that the lesion is:\"\n",
        "    ax[i,1].text(x=-.15, y=.9, s=confidence_text, fontsize=10, fontweight=\"normal\", verticalalignment='top') # Confidence\n",
        "    ax[i,1].text(x=-.15, y=.82, s=full_name, fontsize=10, fontweight=\"bold\", verticalalignment='top') # Classification\n",
        "    ax[i,1].text(x=-.15, y=.75, s=description, fontsize=10, fontweight=\"normal\", verticalalignment='top') # Description\n",
        "\n",
        "    # Display secondary classification information\n",
        "    second_confidence_text = f\"The next most probable classification with {second_confidence}% confidence would be:\"\n",
        "    ax[i,1].text(x=-.15, y=.35, s=second_confidence_text, fontsize=9, fontweight=\"normal\", verticalalignment='top') # 2nd Confidence\n",
        "    ax[i,1].text(x=-.15, y=.30, s=second_full_name, fontsize=9, fontweight=\"bold\", verticalalignment='top') # 2nd Classification\n",
        "    ax[i,1].text(x=-.15, y=.25, s=second_description, fontsize=9, fontweight=\"normal\", verticalalignment='top') # 2nd Description\n",
        "    ax[i,1].axis(False)"
      ],
      "metadata": {
        "id": "neIgutGwruwz"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frontend"
      ],
      "metadata": {
        "id": "hNomIb1V-h75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drag your images to the **Files** panel on the left and run the cell below"
      ],
      "metadata": {
        "id": "l7RUqNX1AXER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diagnose_lesions()"
      ],
      "metadata": {
        "id": "BU0ELtnSruue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}